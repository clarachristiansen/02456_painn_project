{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPLYING PAINN: Molecular Property Prediction Using GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By s214983, s214659, s204618, s183624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code inspiration from the following sources:\n",
    "- Cosine cutoff: https://schnetpack.readthedocs.io/en/latest/_modules/schnetpack/nn/cutoff.html#CosineCutoff\n",
    "- RBF: https://schnetpack.readthedocs.io/en/latest/_modules/schnetpack/nn/radial.html#GaussianRBF\n",
    "- Reduced LR on Plateau: https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n",
    "- PaiNN: https://github.com/nityasagarjena/PaiNN-model/tree/main\n",
    "- Wandb: https://wandb.ai\n",
    "- Edge: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.radius_graph.html?highlight=radius_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QM9 Datamodule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to load the QM9 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: dlopen(/Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.12/Python\n",
      "  Referenced from: <E87A820F-D734-3F45-AFBE-9D80043A97C0> /Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/libpyg.so, 0x0006): Library not loaded: /Library/Frameworks/Python.framework/Versions/3.12/Python\n",
      "  Referenced from: <E87A820F-D734-3F45-AFBE-9D80043A97C0> /Users/clarasofiechristiansen/anaconda3/envs/painn_venv4/lib/python3.12/site-packages/libpyg.so\n",
      "  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file), '/Library/Frameworks/Python.framework/Versions/3.12/Python' (no such file)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.datasets import QM9\n",
    "from torch_geometric.loader import DataLoader\n",
    "from typing import Optional, List, Union, Tuple\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class GetTarget(BaseTransform):\n",
    "    def __init__(self, target: Optional[int] = None) -> None:\n",
    "        self.target = [target]\n",
    "\n",
    "\n",
    "    def forward(self, data: Data) -> Data:\n",
    "        if self.target is not None:\n",
    "            data.y = data.y[:, self.target]\n",
    "        return data\n",
    "\n",
    "\n",
    "class QM9DataModule(pl.LightningDataModule):\n",
    "\n",
    "    target_types = ['atomwise' for _ in range(19)]\n",
    "    target_types[0] = 'dipole_moment'\n",
    "    target_types[5] = 'electronic_spatial_extent'\n",
    "\n",
    "    # Specify unit conversions (eV to meV).\n",
    "    unit_conversion = {\n",
    "        i: (lambda t: 1000*t) if i not in [0, 1, 5, 11, 16, 17, 18]\n",
    "        else (lambda t: t)\n",
    "        for i in range(19)\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target: int = 7,\n",
    "        data_dir: str = 'data/',\n",
    "        batch_size_train: int = 100,\n",
    "        batch_size_inference: int = 1000,\n",
    "        num_workers: int = 0,\n",
    "        splits: Union[List[int], List[float]] = [110000, 10000, 10831],\n",
    "        seed: int = 0,\n",
    "        subset_size: Optional[int] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size_train = batch_size_train\n",
    "        self.batch_size_inference = batch_size_inference\n",
    "        self.num_workers = num_workers\n",
    "        self.splits = splits\n",
    "        self.seed = seed\n",
    "        self.subset_size = subset_size\n",
    "\n",
    "        self.data_train = None\n",
    "        self.data_val = None\n",
    "        self.data_test = None\n",
    "\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download data\n",
    "        QM9(root=self.data_dir)\n",
    "\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        dataset = QM9(root=self.data_dir, transform=GetTarget(self.target))\n",
    "\n",
    "        # Shuffle dataset\n",
    "        rng = np.random.default_rng(seed=self.seed)\n",
    "        dataset = dataset[rng.permutation(len(dataset))]\n",
    "\n",
    "        # Subset dataset\n",
    "        if self.subset_size is not None:\n",
    "            dataset = dataset[:self.subset_size]\n",
    "        \n",
    "        # Split dataset\n",
    "        if all([type(split) == int for split in self.splits]):\n",
    "            split_sizes = self.splits\n",
    "        elif all([type(split) == float for split in self.splits]):\n",
    "            split_sizes = [int(len(dataset) * prop) for prop in self.splits]\n",
    "\n",
    "        split_idx = np.cumsum(split_sizes)\n",
    "        self.data_train = dataset[:split_idx[0]]\n",
    "        self.data_val = dataset[split_idx[0]:split_idx[1]]\n",
    "        self.data_test = dataset[split_idx[1]:]\n",
    "\n",
    "\n",
    "    def get_target_stats(\n",
    "        self,\n",
    "        remove_atom_refs: bool = True,\n",
    "        divide_by_atoms: bool = True\n",
    "    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]:\n",
    "        atom_refs = self.data_train.atomref(self.target)\n",
    "\n",
    "        ys = list()\n",
    "        for batch in self.train_dataloader(shuffle=False):\n",
    "            y = batch.y.clone()\n",
    "            if remove_atom_refs and atom_refs is not None:\n",
    "                y.index_add_(\n",
    "                    dim=0, index=batch.batch, source=-atom_refs[batch.z]\n",
    "                )\n",
    "            if divide_by_atoms:\n",
    "                _, num_atoms  = torch.unique(batch.batch, return_counts=True)\n",
    "                y = y / num_atoms.unsqueeze(-1)\n",
    "            ys.append(y)\n",
    "\n",
    "        y = torch.cat(ys, dim=0)\n",
    "        return y.mean(), y.std(), atom_refs\n",
    "\n",
    "\n",
    "    def train_dataloader(self, shuffle: bool = True) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_train,\n",
    "            batch_size=self.batch_size_train,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=shuffle,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_val,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.data_test,\n",
    "            batch_size=self.batch_size_inference,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to process the output of the PaiNN model. The functions sums the atomic contributions predicted by the model across the molecules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class AtomwisePostProcessing(nn.Module):\n",
    "    \"\"\"\n",
    "    Post-processing for (QM9) properties that are predicted as sums of atomic\n",
    "    contributions.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_outputs: int,\n",
    "        mean: torch.FloatTensor,\n",
    "        std: torch.FloatTensor,\n",
    "        atom_refs: torch.FloatTensor,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_outputs: Integer with the number of model outputs. In most\n",
    "                cases 1.\n",
    "            mean: torch.FloatTensor with mean value to shift atomwise\n",
    "                contributions by.\n",
    "            std: torch.FloatTensor with standard deviation to scale atomwise\n",
    "                contributions by.\n",
    "            atom_refs: torch.FloatTensor of size [num_atom_types, 1] with\n",
    "                atomic reference values.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "        self.register_buffer('scale', std)\n",
    "        self.register_buffer('shift', mean)\n",
    "        self.atom_refs = nn.Embedding.from_pretrained(atom_refs, freeze=True)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atomic_contributions: torch.FloatTensor,\n",
    "        atoms: torch.LongTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Atomwise post-processing operations and atomic sum.\n",
    "\n",
    "        Args:\n",
    "            atomic_contributions: torch.FloatTensor of size [num_nodes,\n",
    "                num_outputs] with each node's contribution to the overall graph\n",
    "                prediction, i.e., each atom's contribution to the overall\n",
    "                molecular property prediction.\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FLoatTensor of size [num_graphs, num_outputs] with\n",
    "            predictions for each graph (molecule).\n",
    "        \"\"\"\n",
    "        num_graphs = torch.unique(graph_indexes).shape[0]\n",
    "\n",
    "        atomic_contributions = atomic_contributions*self.scale + self.shift\n",
    "        atomic_contributions = atomic_contributions + self.atom_refs(atoms)\n",
    "\n",
    "        # Sum contributions for each graph\n",
    "        output_per_graph = torch.zeros(\n",
    "            (num_graphs, self.num_outputs),\n",
    "            device=atomic_contributions.device,\n",
    "        )\n",
    "        output_per_graph.index_add_(\n",
    "            dim=0,\n",
    "            index=graph_indexes,\n",
    "            source=atomic_contributions,\n",
    "        )\n",
    "\n",
    "        return output_per_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli(args: list = []):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=3, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args(args=args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "device = 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model architecture found with layer optimization. It has two message-update blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import radius_graph\n",
    "from schnetpack.nn.radial import BesselRBF\n",
    "from schnetpack.nn.cutoff import CosineCutoff\n",
    "\n",
    "class MessageBlock(nn.Module):\n",
    "    def __init__(self, num_features, num_rbf_features, cutoff_dist):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "\n",
    "        # Message\n",
    "        self.cutoff_function = CosineCutoff(self.cutoff_dist)\n",
    "\n",
    "        self.phi_path = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.num_features, self.num_features * 3))\n",
    "        self.W_path = nn.Sequential(\n",
    "            nn.Linear(self.num_rbf_features, self.num_features * 3))\n",
    "        \n",
    "        # Update\n",
    "        self.U = nn.Linear(self.num_features, self.num_features)\n",
    "        self.V = nn.Linear(self.num_features, self.num_features)\n",
    "\n",
    "        self.mlp_update = nn.Sequential(\n",
    "            nn.Linear(self.num_features * 2, self.num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.num_features, self.num_features * 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, s, v, i_index, j_index, rbf, r_ij_direction):\n",
    "        # Message \n",
    "        phi = self.phi_path(s)\n",
    "        W = self.cutoff_function(self.W_path(rbf))\n",
    "        split = phi[j_index] * W # i_index\n",
    "        Ws, Wvs, Wvv = torch.split(split, self.num_features, dim=-1)\n",
    "        \n",
    "        delta_v_all = Wvs.unsqueeze(-1) * r_ij_direction.unsqueeze(1) + Wvv.unsqueeze(-1) * v[j_index] # right and left path\n",
    "        \n",
    "        delta_v = torch.zeros_like(v)\n",
    "        delta_v = delta_v.index_add_(0, i_index, delta_v_all)\n",
    "        v = v + delta_v\n",
    "\n",
    "        delta_s = torch.zeros_like(s)\n",
    "        delta_s = delta_s.index_add_(0, i_index, Ws)\n",
    "        s = s + delta_s\n",
    "\n",
    "        # Update \n",
    "        v_permuted = torch.permute(v, (0,2,1))\n",
    "        Uv = torch.permute(self.U(v_permuted), (0,2,1))\n",
    "        Vv = torch.permute(self.V(v_permuted), (0,2,1))\n",
    "        Vv_norm = torch.linalg.norm(Vv, dim=2)\n",
    "        mlp_input = torch.hstack([Vv_norm, s])\n",
    "        mlp_result = self.mlp_update(mlp_input)\n",
    "\n",
    "        a_vv, a_sv, a_ss = torch.split(mlp_result, self.num_features, dim=-1)\n",
    "        \n",
    "        dv = a_vv.unsqueeze(-1) * Uv\n",
    "        \n",
    "        dot_prod = torch.sum(Uv * Vv, dim=2) # dot product\n",
    "        ds = dot_prod * a_sv + a_ss\n",
    "        \n",
    "        s = s + ds\n",
    "        v = v + delta_v # dv\n",
    "\n",
    "        return s, v\n",
    "\n",
    "\n",
    "\n",
    "class PaiNN1(nn.Module):\n",
    "    \"\"\"\n",
    "    Polarizable Atom Interaction Neural Network with PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_message_passing_layers: int = 3,\n",
    "        num_features: int = 128,\n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "        device: str='cpu',\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_message_passing_layers: Number of message passing layers in\n",
    "                the PaiNN model.\n",
    "            num_features: Size of the node embeddings (scalar features) and\n",
    "                vector features.\n",
    "            num_outputs: Number of model outputs. In most cases 1.\n",
    "            num_rbf_features: Number of radial basis functions to represent\n",
    "                distances.\n",
    "            num_unique_atoms: Number of unique atoms in the data that we want\n",
    "                to learn embeddings for.\n",
    "            cutoff_dist: Euclidean distance threshold for determining whether \n",
    "                two nodes (atoms) are neighbours.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_outputs = num_outputs\n",
    "        self.cutoff_dist = cutoff_dist\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_unique_atoms = num_unique_atoms\n",
    "        self.device = device\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "        # Initial embeddings function\n",
    "        self.embedding = nn.Embedding(self.num_unique_atoms, self.num_features)    \n",
    "        # RBF\n",
    "        self.rbf = BesselRBF(self.num_rbf_features, self.cutoff_dist)\n",
    "\n",
    "        # Message blocks (both message and update)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MessageBlock(num_features, num_rbf_features, self.cutoff_dist) \n",
    "            for _ in range(self.num_message_passing_layers)\n",
    "        ])\n",
    "\n",
    "        # Last MLP\n",
    "        self.last_mlp = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.num_features, self.num_outputs)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        atoms: torch.LongTensor,\n",
    "        atom_positions: torch.FloatTensor,\n",
    "        graph_indexes: torch.LongTensor,\n",
    "    ) -> torch.FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward pass of PaiNN. Includes the readout network highlighted in blue\n",
    "        in Figure 2 in (Schütt et al., 2021) with normal linear layers which is\n",
    "        used for predicting properties as sums of atomic contributions. The\n",
    "        post-processing and final sum is perfomed with\n",
    "        src.models.AtomwisePostProcessing.\n",
    "\n",
    "        Args:\n",
    "            atoms: torch.LongTensor of size [num_nodes] with atom type of each\n",
    "                node in the graph.\n",
    "            atom_positions: torch.FloatTensor of size [num_nodes, 3] with\n",
    "                euclidean coordinates of each node / atom.\n",
    "            graph_indexes: torch.LongTensor of size [num_nodes] with the graph \n",
    "                index each node belongs to.\n",
    "\n",
    "        Returns:\n",
    "            A torch.FloatTensor of size [num_nodes, num_outputs] with atomic\n",
    "            contributions to the overall molecular property prediction.\n",
    "        \"\"\"\n",
    "        s = self.embedding(atoms)\n",
    "        v = torch.zeros((atoms.shape[0], self.num_features, 3), device=self.device) \n",
    "        i_index, j_index = build_edge_index(atom_positions, self.cutoff_dist, graph_indexes)\n",
    "        r_ij = atom_positions[j_index] - atom_positions[i_index] # Check\n",
    "        distance = torch.linalg.norm(r_ij, axis=1, keepdim=True)\n",
    "        #distance = torch.clamp(distance, min=1e-8)\n",
    "        rbf = self.rbf(distance.squeeze())\n",
    "        r_ij_direction = r_ij / (distance + 1e-8)\n",
    "        # Message passing\n",
    "        for block in self.blocks:\n",
    "            s, v = block(s, v, i_index, j_index, rbf, r_ij_direction)\n",
    "        \n",
    "        E = self.last_mlp(s)\n",
    "        #print(E)\n",
    "        return E\n",
    "        \n",
    "\n",
    "def build_edge_index(atom_positions, cutoff_distance, graph_indexes):\n",
    "    edge_index =radius_graph(atom_positions, r=cutoff_distance, batch=graph_indexes, flow='target_to_source')\n",
    "    #print(edge_index)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/l3776shn2kq8j4f7hm3qr5bh0000gn/T/ipykernel_5748/1596419244.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  painn.load_state_dict(torch.load('model1_p5mqh3xg.pth', map_location=torch.device(device)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "# Trained with the settings: Early stopping, reduced learning rate on plateau\n",
    "painn = PaiNN1(num_message_passing_layers=2)\n",
    "painn.load_state_dict(torch.load('model1_p5mqh3xg.pth', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes less than 5 minutes on CPU (including loading the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded\n",
      "Target Stats Loaded\n",
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Test MAE: 17.992\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "\n",
    "dm = QM9DataModule(\n",
    "    target=args.target,\n",
    "    data_dir=args.data_dir,\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_inference=args.batch_size_inference,\n",
    "    num_workers=args.num_workers,\n",
    "    splits=args.splits,\n",
    "    seed=args.seed,\n",
    "    subset_size=args.subset_size,\n",
    ")\n",
    "dm.prepare_data()\n",
    "print('Data Loaded')\n",
    "dm.setup()\n",
    "y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "    remove_atom_refs=True, divide_by_atoms=True\n",
    ")\n",
    "print('Target Stats Loaded')\n",
    "post_processing = AtomwisePostProcessing(\n",
    "    args.num_outputs, y_mean, y_std, atom_refs\n",
    ")\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dm.test_dataloader()):\n",
    "        print('Batch:', i)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN 1.0 with SWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/l3776shn2kq8j4f7hm3qr5bh0000gn/T/ipykernel_5748/434292759.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  painn_swa.load_state_dict(torch.load('model1_swa_iccjgnmp.pth', map_location=torch.device(device)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model with SWA\n",
    "# Trained with the best model seen above.\n",
    "# Trained with the settings: Early stopping, reduced learning rate on plateau, and SWA \n",
    "painn_swa = PaiNN1(num_message_passing_layers=2)\n",
    "painn_swa.load_state_dict(torch.load('model1_swa_iccjgnmp.pth', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Test MAE: 17.272\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "\n",
    "painn_swa.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "mae = 0\n",
    "painn_swa.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dm.test_dataloader()):\n",
    "        print('Batch:', i)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn_swa(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PaiNN 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch_geometric.nn import radius_graph\n",
    "\n",
    "def rbf_generator(distance, num_rbf_features, cutoff_distance):\n",
    "    n = torch.arange(num_rbf_features, device=distance.device) + 1\n",
    "    return torch.sin(distance.unsqueeze(-1) * n * torch.pi / cutoff_distance) / distance.unsqueeze(-1)\n",
    "\n",
    "def f_cut(distance, cutoff_distance):\n",
    "    # https://schnetpack.readthedocs.io/en/latest/_modules/schnetpack/nn/cutoff.html#CosineCutoff\n",
    "    return torch.where(\n",
    "        distance < cutoff_distance,\n",
    "        0.5 * (torch.cos(torch.pi * distance / cutoff_distance) + 1),\n",
    "        torch.tensor(0.0, device=distance.device, dtype=distance.dtype),\n",
    "    )\n",
    "\n",
    "class MessageLayer(nn.Module):\n",
    "    def __init__(self, num_features, num_rbf_features, cutoff_distance):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.cutoff_distance = cutoff_distance\n",
    "        \n",
    "        self.phi_path = nn.Sequential(\n",
    "            nn.Linear(num_features, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_features * 3),\n",
    "        )\n",
    "        \n",
    "        self.W_path = nn.Linear(num_rbf_features, num_features * 3)\n",
    "        \n",
    "    def forward(self, s, v, edge_indexes, r_ij, distance):\n",
    "        W = self.W_path(rbf_generator(distance, self.num_rbf_features, self.cutoff_distance))\n",
    "        W = W * f_cut(distance, self.cutoff_distance).unsqueeze(-1)\n",
    "        phi = self.phi_path(s)        \n",
    "        split = W * phi[edge_indexes[:, 1]]\n",
    "        \n",
    "        Wvv, Wvs, Wvs = torch.split(split, self.num_features, dim = 1)\n",
    "        \n",
    "        v_1 =  v[edge_indexes[:, 1]] * Wvv.unsqueeze(1) \n",
    "        v_2 = Wvs.unsqueeze(1) * (r_ij / distance.unsqueeze(-1)).unsqueeze(-1)\n",
    "        v_sum = v_1 + v_2\n",
    "        \n",
    "        delta_s = torch.zeros_like(s)\n",
    "        delta_s.index_add_(0, edge_indexes[:, 0], Wvs)\n",
    "        new_s = s + delta_s\n",
    "\n",
    "        delta_v = torch.zeros_like(v)\n",
    "        delta_v.index_add_(0, edge_indexes[:, 0], v_sum)\n",
    "        new_v = v + delta_v\n",
    "        \n",
    "        return new_s, new_v\n",
    "\n",
    "class UpdateLayer(nn.Module):\n",
    "    def __init__(self, num_features: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.V = nn.Linear(num_features, num_features)\n",
    "        self.U = nn.Linear(num_features, num_features)\n",
    "        \n",
    "        self.mlp_update = nn.Sequential(\n",
    "            nn.Linear(num_features * 2, num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(num_features, num_features * 3),\n",
    "        )\n",
    "        \n",
    "    def forward(self, s, v):\n",
    "        Vv = self.V(v)\n",
    "        Uv = self.U(v)\n",
    "        \n",
    "        Vv_norm = torch.linalg.norm(Vv, dim=1)\n",
    "        mlp_input = torch.cat((Vv_norm, s), dim=1)\n",
    "        mlp_result = self.mlp_update(mlp_input)\n",
    "        \n",
    "        a_vv, a_sv, a_ss = torch.split(mlp_result, v.shape[-1], dim = 1)\n",
    "        \n",
    "        dot_prod = torch.sum(Uv * Vv, dim=1)\n",
    "        delta_s = a_sv * dot_prod + a_ss\n",
    "        new_s = s + delta_s\n",
    "\n",
    "        delta_v = a_vv.unsqueeze(1) * Uv\n",
    "        new_v = v + delta_v\n",
    "\n",
    "        return new_s, new_v\n",
    "\n",
    "class PaiNN2(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_message_passing_layers: int=5, \n",
    "        num_features: int = 128, \n",
    "        num_outputs: int = 1,\n",
    "        num_rbf_features: int = 20,\n",
    "        num_unique_atoms: int = 100,\n",
    "        cutoff_dist: float = 5.0,\n",
    "        device: str='cpu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_unique_atoms = num_unique_atoms   # number of all elements\n",
    "        self.cutoff_distance = cutoff_dist\n",
    "        self.num_message_passing_layers = num_message_passing_layers\n",
    "        self.num_features = num_features\n",
    "        self.num_rbf_features = num_rbf_features\n",
    "        self.num_outputs = num_outputs\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.num_unique_atoms, self.num_features)\n",
    "\n",
    "        self.message_layers = nn.ModuleList(\n",
    "            [\n",
    "                MessageLayer(self.num_features, self.num_rbf_features, self.cutoff_distance)\n",
    "                for _ in range(self.num_message_passing_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.update_layers = nn.ModuleList(\n",
    "            [\n",
    "                UpdateLayer(self.num_features)\n",
    "                for _ in range(self.num_message_passing_layers)\n",
    "            ]            \n",
    "        )\n",
    "        \n",
    "        self.last_mlp = nn.Sequential(\n",
    "            nn.Linear(self.num_features, self.num_features),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.num_features, self.num_outputs),\n",
    "        )\n",
    "        \n",
    "    def forward(self, atoms, atom_positions, graph_indexes):\n",
    "        edge = build_edge_index(atom_positions, self.cutoff_distance, graph_indexes).T\n",
    "       \n",
    "        r_ij = atom_positions[edge[:,1]] - atom_positions[edge[:,0]]\n",
    "\n",
    "        distance = torch.linalg.norm(r_ij, dim=1)\n",
    "        \n",
    "        s = self.embedding(atoms)\n",
    "        v = torch.zeros((atom_positions.shape[0], 3, self.num_features), device=r_ij.device, dtype=r_ij.dtype)\n",
    "        \n",
    "        for message_layer, update_layer in zip(self.message_layers, self.update_layers):\n",
    "            s, v = message_layer(s, v, edge, r_ij, distance)\n",
    "            s, v = update_layer(s, v)\n",
    "        \n",
    "        s = self.last_mlp(s)\n",
    "        \n",
    "        return s\n",
    "\n",
    "def build_edge_index(atom_positions, cutoff_distanceance, graph_indexes):\n",
    "    edge_index =radius_graph(atom_positions, r=cutoff_distanceance, batch=graph_indexes, flow='target_to_source')\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_6/l3776shn2kq8j4f7hm3qr5bh0000gn/T/ipykernel_5748/433348239.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  painn.load_state_dict(torch.load('model2_dvyocn32.pth', map_location=torch.device(device)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "# Trained with the settings: Early stopping, reduced learning rate on plateau\n",
    "painn = PaiNN2(num_message_passing_layers=5)\n",
    "painn.load_state_dict(torch.load('model2_dvyocn32.pth', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Batch: 1\n",
      "Batch: 2\n",
      "Batch: 3\n",
      "Batch: 4\n",
      "Batch: 5\n",
      "Batch: 6\n",
      "Batch: 7\n",
      "Batch: 8\n",
      "Batch: 9\n",
      "Batch: 10\n",
      "Test MAE: 6.996\n"
     ]
    }
   ],
   "source": [
    "args = [] # Specify non-default arguments in this list\n",
    "args = cli(args)\n",
    "seed_everything(args.seed)\n",
    "\n",
    "painn.to(device)\n",
    "post_processing.to(device)\n",
    "\n",
    "mae = 0\n",
    "painn.eval()\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(dm.test_dataloader()):\n",
    "        print('Batch:', i)\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        atomic_contributions = painn(\n",
    "            atoms=batch.z,\n",
    "            atom_positions=batch.pos,\n",
    "            graph_indexes=batch.batch,\n",
    "        )\n",
    "        preds = post_processing(\n",
    "            atoms=batch.z,\n",
    "            graph_indexes=batch.batch,\n",
    "            atomic_contributions=atomic_contributions,\n",
    "        )\n",
    "        mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "\n",
    "mae /= len(dm.data_test)\n",
    "unit_conversion = dm.unit_conversion[args.target]\n",
    "print(f'Test MAE: {unit_conversion(mae):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "```python\n",
    "\"\"\"\n",
    "Basic example of how to train the PaiNN model to predict the QM9 property\n",
    "\"internal energy at 0K\". This property (and the majority of the other QM9\n",
    "properties) is computed as a sum of atomic contributions.\n",
    "\"\"\"\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import trange\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from src.data import QM9DataModule\n",
    "from pytorch_lightning import seed_everything\n",
    "from src.models import AtomwisePostProcessing\n",
    "from src.models.painn import PaiNN# this is the working one!\n",
    "import wandb\n",
    "\n",
    "print('currrent working directory: ', os.getcwd())\n",
    "load = True\n",
    "def cli():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--seed', default=0)\n",
    "\n",
    "    # Data\n",
    "    parser.add_argument('--target', default=7, type=int) # 7 => Internal energy at 0K\n",
    "    parser.add_argument('--data_dir', default='data/', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=100, type=int)\n",
    "    parser.add_argument('--batch_size_inference', default=1000, type=int)\n",
    "    parser.add_argument('--num_workers', default=0, type=int)\n",
    "    parser.add_argument('--splits', nargs=3, default=[110000, 10000, 10831], type=int) # [num_train, num_val, num_test]\n",
    "    parser.add_argument('--subset_size', default=None, type=int)\n",
    "\n",
    "    # Model\n",
    "    parser.add_argument('--num_message_passing_layers', default=6, type=int)\n",
    "    parser.add_argument('--num_features', default=128, type=int)\n",
    "    parser.add_argument('--num_outputs', default=1, type=int)\n",
    "    parser.add_argument('--num_rbf_features', default=20, type=int)\n",
    "    parser.add_argument('--num_unique_atoms', default=100, type=int)\n",
    "    parser.add_argument('--cutoff_dist', default=5.0, type=float)\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--num_epochs', default=1000, type=int)\n",
    "\n",
    "    #parser.add_argument('--clip_value', default=1000, type=int)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return 'cuda'\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return 'mps'\n",
    "    else:\n",
    "        return 'cpu'\n",
    "\n",
    "device = get_device()\n",
    "print(device)\n",
    "\n",
    "wandb.login(key='eff5a31d6dfda82af022ae7c5286724a57c42f8c')\n",
    "# Initialize wandb sweep config\n",
    "sweep_config = {\n",
    "    'method': 'grid',  # Options: 'random', 'grid', 'bayes'\n",
    "    'metric': {\n",
    "        'name': 'val_loss',\n",
    "        'goal': 'minimize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        #'num_message_passing_layers': {'values': [1]}, # [2, 3, 4, 5]\n",
    "        #'clip_value': {'values': [1, 10, 100]}\n",
    "        'lr': {'values': [5e-4, 4]}, # [1e-3, 5e-4, 1e-4]\n",
    "        #'batch_size_train': {'values': [32, 64, 100]},\n",
    "        #'weight_decay': {'values': [0.01]} # [0.01, 0.001, 0.0001]\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"layeropt_6\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Parse static arguments\n",
    "    args = cli()\n",
    "    seed_everything(args.seed)\n",
    "\n",
    "    # wandb configuration (dynamic hyperparameters)\n",
    "    with wandb.init(config=args.__dict__) as run:\n",
    "        config = wandb.config  # Access wandb-specified parameters\n",
    "        print(wandb.run.id)\n",
    "\n",
    "        # Use a mix of CLI and wandb settings\n",
    "        dm = QM9DataModule(\n",
    "            target=args.target,\n",
    "            data_dir=args.data_dir,\n",
    "            batch_size_train=config.get(\"batch_size_train\", args.batch_size_train),\n",
    "            batch_size_inference=args.batch_size_inference,\n",
    "            num_workers=args.num_workers,\n",
    "            splits=args.splits,\n",
    "            seed=args.seed,\n",
    "            subset_size=args.subset_size,\n",
    "        )\n",
    "        dm.prepare_data()\n",
    "        dm.setup()\n",
    "        y_mean, y_std, atom_refs = dm.get_target_stats(\n",
    "        remove_atom_refs=True, divide_by_atoms=True\n",
    "        )\n",
    "        painn = PaiNN(\n",
    "            num_message_passing_layers=config.get(\"num_message_passing_layers\", args.num_message_passing_layers),\n",
    "            num_features=args.num_features,\n",
    "            num_outputs=args.num_outputs,\n",
    "            num_rbf_features=args.num_rbf_features,\n",
    "            num_unique_atoms=args.num_unique_atoms,\n",
    "            cutoff_dist=args.cutoff_dist,\n",
    "            device=device,\n",
    "        )\n",
    "        post_processing = AtomwisePostProcessing(\n",
    "        args.num_outputs, y_mean, y_std, atom_refs\n",
    "        )\n",
    "        painn.to(device)\n",
    "        post_processing.to(device)\n",
    "\n",
    "        # Optimizer hyperparameters come from wandb\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            painn.parameters(),\n",
    "            lr=config.get(\"lr\", args.lr),\n",
    "            weight_decay=config.get(\"weight_decay\", args.weight_decay),\n",
    "        )\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "        # Early stopping setup\n",
    "        smoothed_loss = 0.0\n",
    "        best_val_loss_smooth = float('inf')\n",
    "        patience = 30  # Stop training if no improvement after 20 epochs\n",
    "        patience_counter = 0\n",
    "        alpha = 0.9\n",
    "        \n",
    "        pbar = trange(args.num_epochs)\n",
    "        for epoch in pbar:\n",
    "            painn.train()\n",
    "            train_loss = 0.0\n",
    "            train_mae = 0.0  # Initialize train MAE accumulator\n",
    "\n",
    "            for batch in dm.train_dataloader():\n",
    "                batch = batch.to(device)\n",
    "                atomic_contributions = painn(\n",
    "                    atoms=batch.z,\n",
    "                    atom_positions=batch.pos,\n",
    "                    graph_indexes=batch.batch\n",
    "                )\n",
    "                preds = post_processing(\n",
    "                    atoms=batch.z,\n",
    "                    graph_indexes=batch.batch,\n",
    "                    atomic_contributions=atomic_contributions,\n",
    "                )\n",
    "                loss = F.mse_loss(preds, batch.y, reduction='sum') / len(batch.y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                #if epoch < 10: torch.nn.utils.clip_grad_norm_(painn.parameters(), max_norm=args.clip_value) \n",
    "                #else: \n",
    "                torch.nn.utils.clip_grad_value_(painn.parameters(), clip_value=config.get(\"clip_value\", 100))\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                # Compute MAE\n",
    "                mae_step = F.l1_loss(preds, batch.y, reduction='sum').item()\n",
    "                train_mae += mae_step\n",
    "\n",
    "            train_loss /= len(dm.data_train)\n",
    "            train_mae /= len(dm.data_train)  # Normalize MAE by dataset size\n",
    "\n",
    "            # Validation Loop\n",
    "            painn.eval()\n",
    "            val_loss = 0.0\n",
    "            val_mae = 0.0  # Initialize val MAE accumulator\n",
    "            with torch.no_grad():\n",
    "                for batch in dm.val_dataloader():\n",
    "                    batch = batch.to(device)\n",
    "                    atomic_contributions = painn(\n",
    "                        atoms=batch.z,\n",
    "                        atom_positions=batch.pos,\n",
    "                        graph_indexes=batch.batch,\n",
    "                    )\n",
    "                    preds = post_processing(\n",
    "                        atoms=batch.z,\n",
    "                        graph_indexes=batch.batch,\n",
    "                        atomic_contributions=atomic_contributions,\n",
    "                    )\n",
    "                    val_loss += F.mse_loss(preds, batch.y, reduction='sum').item()\n",
    "                    \n",
    "                    # Compute MAE\n",
    "                    mae_step = F.l1_loss(preds, batch.y, reduction='sum').item()\n",
    "                    val_mae += mae_step\n",
    "\n",
    "            val_loss /= len(dm.data_val)\n",
    "            val_mae /= len(dm.data_val)  # Normalize MAE by dataset size\n",
    "            \n",
    "            smoothed_loss = alpha * val_loss + (1 - alpha) * smoothed_loss #  Smooth loss\n",
    "            scheduler.step(smoothed_loss) # update rlr\n",
    "            pbar.set_postfix_str(f'Train loss: {train_loss:.3e}, Val loss: {val_loss:.3e}')\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\n",
    "                'epoch': epoch,\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'train_mae': train_mae,\n",
    "                'val_mae': val_mae,\n",
    "            })\n",
    "\n",
    "            # Early stopping\n",
    "            if (best_val_loss_smooth - smoothed_loss) > 0.0000001:\n",
    "                best_val_loss_smooth = smoothed_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    break\n",
    "\n",
    "# Test evaluation\n",
    "        mae = 0\n",
    "        painn.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dm.test_dataloader():\n",
    "                batch = batch.to(device)\n",
    "                atomic_contributions = painn(\n",
    "                    atoms=batch.z,\n",
    "                    atom_positions=batch.pos,\n",
    "                    graph_indexes=batch.batch,\n",
    "                )\n",
    "                preds = post_processing(\n",
    "                    atoms=batch.z,\n",
    "                    graph_indexes=batch.batch,\n",
    "                    atomic_contributions=atomic_contributions,\n",
    "                )\n",
    "                mae += F.l1_loss(preds, batch.y, reduction='sum')\n",
    "        mae /= len(dm.data_test)\n",
    "        unit_conversion = dm.unit_conversion[args.target]\n",
    "        wandb.log({'Test MAE': unit_conversion(mae.item())})\n",
    "        #print(os.getcwd() + '/src/results/model_{wandb.run.id}.pth')\n",
    "        torch.save(painn.state_dict(), f'./src/results/model_{wandb.run.id}.pth')\n",
    "\n",
    "# Run the sweep agent\n",
    "wandb.agent(sweep_id, function=main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "painn_venv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
